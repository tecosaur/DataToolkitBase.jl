var documenterSearchIndex = {"docs":
[{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"EditURL=\"advising.org\"","category":"page"},{"location":"advising/#Data-Advising","page":"Data Advice","title":"Data Advising","text":"","category":"section"},{"location":"advising/#Advice","page":"Data Advice","title":"Advice","text":"","category":"section"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"Inspired by Lisp, DataToolkitCore comes with a method of completely transforming its behaviour at certain defined points. This is essentially a restricted form of Aspect-oriented programming. At certain declared locations (termed \"join points\"), we consult a list of \"advise\" functions that modify the execution at that point, and apply the (matched via \"pointcuts\") advise functions accordingly.","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"(Image: image)","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"Each applied advise function is wrapped around the invocation of the join point, and is able to modify the arguments, execution, and results of the join point.","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"(Image: image)","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"Advice","category":"page"},{"location":"advising/#DataToolkitCore.Advice","page":"Data Advice","title":"DataToolkitCore.Advice","text":"Advice{func, context} <: Function\n\nAdvices allow for composable, highly flexible modifications of data by encapsulating a function call. They are inspired by elisp's advice system, namely the most versatile form — :around advice, and Clojure's advisors.\n\nA Advice is essentially a function wrapper, with a priority::Int attribute. The wrapped functions should be of the form:\n\n(action::Function, args...; kargs...) ->\n  ([post::Function], action::Function, args::Tuple, [kargs::NamedTuple])\n\nShort-hand return values with post or kargs omitted are also accepted, in which case default values (the identity function and (;) respectively) will be automatically substituted in.\n\n    input=(action args kwargs)\n         ┃                 ┏╸post=identity\n       ╭─╂────advisor 1────╂─╮\n       ╰─╂─────────────────╂─╯\n       ╭─╂────advisor 2────╂─╮\n       ╰─╂─────────────────╂─╯\n       ╭─╂────advisor 3────╂─╮\n       ╰─╂─────────────────╂─╯\n         ┃                 ┃\n         ▼                 ▽\naction(args; kargs) ━━━━▶ post╺━━▶ result\n\nTo specify which transforms a Advice should be applied to, ensure you add the relevant type parameters to your transducing function. In cases where the transducing function is not applicable, the Advice will simply act as the identity function.\n\nAfter all applicable Advices have been applied, action(args...; kargs...) |> post is called to produce the final result.\n\nThe final post function is created by rightwards-composition with every post entry of the advice forms (i.e. at each stage post = post ∘ extra is run).\n\nThe overall behaviour can be thought of as shells of advice.\n\n        ╭╌ advisor 1 ╌╌╌╌╌╌╌╌─╮\n        ┆ ╭╌ advisor 2 ╌╌╌╌╌╮ ┆\n        ┆ ┆                 ┆ ┆\ninput ━━┿━┿━━━▶ function ━━━┿━┿━━▶ result\n        ┆ ╰╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╯ ┆\n        ╰╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╯\n\nConstructors\n\nAdvice(priority::Int, f::Function)\nAdvice(f::Function) # priority is set to 1\n\nExamples\n\n1. Logging every time a DataSet is loaded.\n\nloggingadvisor = Advice(\n    function(post::Function, f::typeof(load), loader::DataLoader, input, outtype)\n        @info \"Loading $(loader.data.name)\"\n        (post, f, (loader, input, outtype))\n    end)\n\n2. Automatically committing each data file write.\n\nwritecommitadvisor = Advice(\n    function(post::Function, f::typeof(write), writer::DataWriter{:filesystem}, output, info)\n        function writecommit(result)\n            run(`git add $output`)\n            run(`git commit -m \"update $output\"`)\n            result\n        end\n        (post ∘ writecommit, writefn, (output, info))\n    end)\n\n\n\n\n\n","category":"type"},{"location":"advising/#Advisement-points-(standard-join-points)","page":"Data Advice","title":"Advisement points (standard join points)","text":"","category":"section"},{"location":"advising/#Parsing-and-serialisation-of-data-sets-and-collections","page":"Data Advice","title":"Parsing and serialisation of data sets and collections","text":"","category":"section"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"DataCollection​s, DataSet​s, and AbstractDataTransformer​s are advised at two stages during parsing:","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"When calling fromspec on the Dict representation, at the start of parsing\nAt the end of the fromspec function, calling identity on the object","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"Serialisation is performed through the tospec call, which is also advised.","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"The signatures of the advised function calls are as follows:","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"fromspec(DataCollection, spec::Dict{String, Any}; path::Union{String, Nothing})::DataCollection\nidentity(collection::DataCollection)::DataCollection\ntospec(collection::DataCollection)::Dict","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"fromspec(DataSet, collection::DataCollection, name::String, spec::Dict{String, Any})::DataSet\nidentity(dataset::DataSet)::DataSet\ntospec(dataset::DataSet)::Dict","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"fromspec(ADT::Type{<:AbstractDataTransformer}, dataset::DataSet, spec::Dict{String, Any})::ADT\nidentity(adt::AbstractDataTransformer)::AbstractDataTransformer\ntospec(adt::AbstractDataTransformer)::Dict","category":"page"},{"location":"advising/#Processing-identifiers","page":"Data Advice","title":"Processing identifiers","text":"","category":"section"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"Both the parsing of an Identifier from a string, and the serialisation of an Identifier to a string are advised. Specifically, the following function calls:","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"parse_ident(spec::AbstractString)\nstring(ident::Identifier)","category":"page"},{"location":"advising/#The-data-flow-arrows","page":"Data Advice","title":"The data flow arrows","text":"","category":"section"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"The reading, writing, and storage of data may all be advised. Specifically, the following function calls:","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"load(loader::DataLoader, datahandle, as::Type)\nstorage(provider::DataStorage, as::Type; write::Bool)\nsave(writer::DataWriter, datahandle, info)","category":"page"},{"location":"advising/#Index-of-advised-calls-(all-known-join-points)","page":"Data Advice","title":"Index of advised calls (all known join points)","text":"","category":"section"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"using Markdown\ncontent = Any[]\n\nconst AdviseRecord = NamedTuple{(:location, :parent, :invocation), Tuple{LineNumberNode, <:Union{Expr, Symbol}, Expr}}\nfunction findadvice!(acc::Vector{AdviseRecord}, expr::Expr; parent=nothing)\n    if expr.head == :macrocall && first(expr.args) == Symbol(\"@advise\")\n        !isnothing(parent) || @warn \"Macro @$(expr.args[2]) has no parent function\"\n        push!(acc, (; location=expr.args[2], parent, invocation=expr.args[end]))\n    else\n        if isnothing(parent) && expr.head == :function\n            parent = if first(expr.args) isa Expr\n                first(first(expr.args).args)\n            else\n                first(expr.args)\n            end\n        elseif isnothing(parent) && expr.head == :(=) &&\n            first(expr.args) isa Expr && first(expr.args).head == :call\n            parent = first(first(expr.args).args)\n        end\n        findadvice!.(Ref(acc), expr.args; parent)\n    end\nend\nfindadvice!(acc, ::Any; parent=nothing) = nothing\n\nalladvice = Vector{AdviseRecord}()\nfor (root, dirs, files) in walkdir(\"../../src\")\n    for file in files\n        file == \"precompile.jl\" && continue\n        @info \"Analysing $file for advise\"\n        path = joinpath(root, file)\n        expr = Meta.parseall(read(path, String); filename=path)\n        findadvice!(alladvice, expr)\n    end\nend\n\nAdvItem = NamedTuple{(:line, :parent, :invocation), Tuple{Int, Union{Expr, Symbol}, Expr}}\nadvbyfunc = Dict{Symbol, Dict{Symbol, Vector{AdvItem}}}()\natypes = first.(getfield.(getfield.(alladvice, :invocation), :args)) |> unique\nafiles = getfield.(getfield.(alladvice, :location), :file) |> unique\n\nfor atype in atypes\n    advs = filter(a -> first(a.invocation.args) == atype, alladvice)\n    advbyfunc[atype] = Dict{Symbol, Vector{AdvItem}}()\n    for (; location, parent, invocation) in advs\n        if !haskey(advbyfunc[atype], location.file)\n            advbyfunc[atype][location.file] = Vector{AdvItem}()\n        end\n        push!(advbyfunc[atype][location.file], (; line=location.line, parent, invocation))\n    end\nend\n\npush!(content, Markdown.Paragraph([\n    \"There are \", Markdown.Bold(string(length(alladvice))),\n    \" advised function calls, across \",\n    Markdown.Bold(string(length(unique(getfield.(getfield.(alladvice, :location), :file))))),\n    \" files, covering \", Markdown.Bold(string(length(advbyfunc))),\n    \" functions (automatically detected).\"]))\n\npush!(content, Markdown.Header{3}([\"Arranged by function\"]))\n\nfor fname in sort(keys(advbyfunc) |> collect)\n    instances = advbyfunc[fname]\n    nadv = sum(length, values(instances))\n    push!(content, Markdown.Header{4}([\n        Markdown.Code(String(fname)),\n        if nadv == 1\n            \" (1 instance)\"\n        else\n            \" ($nadv instances)\"\n        end]))\n    list = Markdown.List(Any[], -1, false)\n    for file in sort(keys(instances) |> collect)\n        details = instances[file]\n        sublist = Markdown.List(Any[], -1, false)\n        for (; line, parent, invocation) in details\n            push!(sublist.items, Markdown.Paragraph(\n                [\"On line \", string(line), \" \",\n                 Markdown.Code(string(invocation)),\n                 \" is advised within a \",\n                 Markdown.Code(string(parent)), \" method.\"]))\n        end\n        push!(list.items, Any[\n            Markdown.Paragraph([Markdown.Italic(last(splitpath(String(file))))]),\n            sublist])\n    end\n    push!(content, list)\nend\n\npush!(content, Markdown.Header{3}([\"Arranged by file\"]))\n\nadvbyfile = Dict{Symbol, Vector{AdvItem}}()\nfor (; location, parent, invocation) in alladvice\n    if !haskey(advbyfile, location.file)\n        advbyfile[location.file] = Vector{AdvItem}()\n    end\n    push!(advbyfile[location.file], (; line=location.line, parent, invocation))\nend\n\nfor file in sort(afiles)\n    instances = advbyfile[file]\n    push!(content, Markdown.Header{5}([\n        Markdown.Code(last(splitpath(String(file)))),\n        if length(instances) == 1\n            \" (1 instance)\"\n        else\n            \" ($(length(instances)) instances)\"\n        end]))\n    list = Markdown.List(Any[], -1, false)\n    for (; line, parent, invocation) in instances\n        push!(list.items, [Markdown.Paragraph(\n            [\"On line \", string(line), \" \",\n             Markdown.Code(string(invocation)),\n             \" is advised within a \",\n             Markdown.Code(string(parent)), \" method.\"])])\n    end\n    push!(content, list)\nend\n\nMarkdown.MD(content) |> string |> Markdown.parse","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"EditURL=\"datatoml.org\"","category":"page"},{"location":"datatoml/#Data.toml","page":"Data.toml","title":"Data.toml","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"A collection of data sets may be encapsulated in a Data.toml file, the structure of which is described here.","category":"page"},{"location":"datatoml/#Overall-structure","page":"Data.toml","title":"Overall structure","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"data_config_version=0\n\nname=\"data collection name\"\nuuid=\"a UUIDv4\"\nplugins=[\"plugin1\", \"plugin2\", ...]\n\n[config]\n# [Properties of the data collection itself]\n\n[[mydataset]]\nuuid=\"a UUIDv4\"\n# other properties...\n\n[[mydataset.TRANSFORMER]]\ndriver=\"transformer driver\"\ntype=[\"a QualifiedType\", ...]\npriority=1 # (optional)\n# other properties...\n\n[[mydataset]]\n# There may be multiple data sets by the same name,\n# but they must be uniquely identifyable by their properties\n\n[[exampledata]]\n# Another data set","category":"page"},{"location":"datatoml/#Attributes-of-the-data-collection","page":"Data.toml","title":"Attributes of the data collection","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"There are four top-level non-table properties currently recognised.","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"-data_config_version :: The (integer) version of the format. Currently 0   as this project is still in the alpha phase of development, moving towards   beta. -name :: an identifying string. Cannot contain :, and characters outside   of [A-Za-z0-9_] are recommended against. -uuid :: a UUIDv4 used to uniquely refer to the data collection, should it be   renamed etc. -plugins :: a list of plugins which should be used when working with this   data collection","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"In addition to these four, a special table of the name config is recognised. This holds custom attributes of the data collection, e.g.","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"[config]\nmykey=\"value\"\n\n[config.defaults]\ndescription=\"Ooops, somebody forgot to describe this.\"\n\n[config.defaults.storage.filesystem]\npriority=2","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"Note that as a consequence of this special table, no data set may be named \"config\".","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"DataToolkitCore reserves exactly one config attribute: locked. This is used to indicate that the Data.toml file should not be modified, and to override it the attribute must be changed within the Data.toml file. By setting config.locked = true, you protecct yourself from accidental modifications to the data file.","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"This functionality is provided here rather than in DataToolkitsCommon etc. because it supported via the implementation of Base.iswritable(::DataCollection), and so downstream packages would only be able to support this by overriding this method.","category":"page"},{"location":"datatoml/#Structure-of-a-data-set","page":"Data.toml","title":"Structure of a data set","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"[[mydataset]]\nuuid=\"a UUIDv4\"\n# other properties...\n\n[[mydataset.TRANSFORMER]]\ndriver=\"transformer driver\"\ntype=[\"a QualifiedType\", ...] # probably optional\ntype=\"a QualifiedType\" # single-value alternative form\npriority=1 # (optional)\n# other properties...","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"A data set is a top-level instance of an array of tables, with any name other than config. Data set names need not be unique, but should be able to be uniquely identified by the combination of their name and parameters.","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"Apart from data transformers, there is one recognised data property: uuid, a UUIDv4 string. Any number of additional properties may be given (so long as they do not conflict with the transformer names), they may have special behaviour based on plugins or extensions loaded, but will not be treated specially by DataToolkitCore.","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"A data set can have any number of data transformers, but at least two are needed for a functional data set. Data transformers are instances of an array of tables (like data sets), but directly under the data set table.","category":"page"},{"location":"datatoml/#Structure-of-a-data-transformer","page":"Data.toml","title":"Structure of a data transformer","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"There are three data transformers types, with the following names:","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"storage\nloader\nwriter","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"All transformers recognise three properties:","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"driver, the transformer driver name, as a string\ntype, a single QualifiedType string, or an array of them\npriority, an integer which sets the order in which multiple transformers should be considered","category":"page"},{"location":"datatoml/","page":"Data.toml","title":"Data.toml","text":"The driver property is mandatory. type and priority can be omitted, in which case they will adopt the default values. The default type value is either determined dynamically from the available methods, or set for that particular transformer.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"EditURL=\"usage.org\"","category":"page"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/#Identifying-a-dataset","page":"Usage","title":"Identifying a dataset","text":"","category":"section"},{"location":"usage/#Reading-datasets","page":"Usage","title":"Reading datasets","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"read","category":"page"},{"location":"usage/#Base.read","page":"Usage","title":"Base.read","text":"read(filename::AbstractString, DataCollection; writer::Union{Function, Nothing})\n\nRead the entire contents of a file as a DataCollection.\n\nThe default value of writer is self -> write(filename, self).\n\n\n\n\n\nread(io::IO, DataCollection; path::Union{String, Nothing}=nothing, mod::Module=Base.Main)\n\nRead the entirety of io, as a DataCollection.\n\n\n\n\n\nread(dataset::DataSet, as::Type)\nread(dataset::DataSet) # as default type\n\nObtain information from dataset in the form of as, with the appropriate loader and storage provider automatically determined.\n\nThis executes this component of the overall data flow:\n\n                 ╭────loader─────╮\n                 ╵               ▼\nStorage ◀────▶ Data          Information\n\nThe loader and storage provider are selected by identifying the highest priority loader that can be satisfied by a storage provider. What this looks like in practice is illustrated in the diagram below.\n\n      read(dataset, Matrix) ⟶ ::Matrix ◀╮\n         ╭───╯        ╰────────────▷┬───╯\n╔═════╸dataset╺══════════════════╗  │\n║ STORAGE      LOADERS           ║  │\n║ (⟶ File)─┬─╮ (File ⟶ String)   ║  │\n║ (⟶ IO)   ┊ ╰─(File ⟶ Matrix)─┬─╫──╯\n║ (⟶ File)┄╯   (IO ⟶ String)   ┊ ║\n║              (IO ⟶ Matrix)╌╌╌╯ ║\n╚════════════════════════════════╝\n\n  ─ the load path used\n  ┄ an option not taken\n\nTODO explain further\n\n\n\n\n\n","category":"function"},{"location":"usage/#Writing-datasets","page":"Usage","title":"Writing datasets","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"write","category":"page"},{"location":"usage/#Base.write","page":"Usage","title":"Base.write","text":"write(dataset::DataSet, info::Any)\n\nTODO write docstring\n\n\n\n\n\n","category":"function"},{"location":"usage/#Accessing-the-raw-data","page":"Usage","title":"Accessing the raw data","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"open","category":"page"},{"location":"usage/#Base.open","page":"Usage","title":"Base.open","text":"open(dataset::DataSet, as::Type; write::Bool=false)\n\nObtain the data of dataset in the form of as, with the appropriate storage provider automatically selected.\n\nA write flag is also provided, to help the driver pick a more appropriate form of as.\n\nThis executes this component of the overall data flow:\n\n                 ╭────loader─────╮\n                 ╵               ▼\nStorage ◀────▶ Data          Information\n\n\n\n\n\n","category":"function"},{"location":"errors/","page":"Errors","title":"Errors","text":"EditURL=\"errors.org\"","category":"page"},{"location":"errors/#Errors","page":"Errors","title":"Errors","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"This package tries to minimise the use of generic errors, and maximise the helpfulness of error messages. To that end, a number of new error types are defined.","category":"page"},{"location":"errors/#Identifier-exceptions","page":"Errors","title":"Identifier exceptions","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"UnresolveableIdentifier","category":"page"},{"location":"errors/#DataToolkitCore.UnresolveableIdentifier","page":"Errors","title":"DataToolkitCore.UnresolveableIdentifier","text":"UnresolveableIdentifier{T}(identifier::Union{String, UUID}, [collection::DataCollection])\n\nNo T (optionally from collection) could be found that matches identifier.\n\nExample occurrences\n\njulia> d\"iirs\"\nERROR: UnresolveableIdentifier: \"iirs\" does not match any available data sets\n  Did you perhaps mean to refer to one of these data sets?\n    ■:iris (75% match)\nStacktrace: [...]\n\njulia> d\"iris::Int\"\nERROR: UnresolveableIdentifier: \"iris::Int\" does not match any available data sets\n  Without the type restriction, however, the following data sets match:\n    dataset:iris, which is available as a DataFrame, Matrix, CSV.File\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"AmbiguousIdentifier","category":"page"},{"location":"errors/#DataToolkitCore.AmbiguousIdentifier","page":"Errors","title":"DataToolkitCore.AmbiguousIdentifier","text":"AmbiguousIdentifier(identifier::Union{String, UUID}, matches::Vector, [collection])\n\nSearching for identifier (optionally within collection), found multiple matches (provided as matches).\n\nExample occurrence\n\njulia> d\"multimatch\"\nERROR: AmbiguousIdentifier: \"multimatch\" matches multiple data sets\n    ■:multimatch [45685f5f-e6ff-4418-aaf6-084b847236a8]\n    ■:multimatch [92be4bda-55e9-4317-aff4-8d52ee6a5f2c]\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#Package-exceptions","page":"Errors","title":"Package exceptions","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"UnregisteredPackage","category":"page"},{"location":"errors/#DataToolkitCore.UnregisteredPackage","page":"Errors","title":"DataToolkitCore.UnregisteredPackage","text":"UnregisteredPackage(pkg::Symbol, mod::Module)\n\nThe package pkg was asked for within mod, but has not been registered by mod, and so cannot be loaded.\n\nExample occurrence\n\njulia> @require Foo\nERROR: UnregisteredPackage: Foo has not been registered by Main, see @addpkg for more information\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"MissingPackage","category":"page"},{"location":"errors/#DataToolkitCore.MissingPackage","page":"Errors","title":"DataToolkitCore.MissingPackage","text":"MissingPackage(pkg::Base.PkgId)\n\nThe package pkg was asked for, but does not seem to be available in the current environment.\n\nExample occurrence\n\njulia> @addpkg Bar \"00000000-0000-0000-0000-000000000000\"\nBar [00000000-0000-0000-0000-000000000000]\n\njulia> @require Bar\n[ Info: Lazy-loading Bar [00000000-0000-0000-0000-000000000001]\nERROR: MissingPackage: Bar [00000000-0000-0000-0000-000000000001] has been required, but does not seem to be installed.\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#Data-Operation-exceptions","page":"Errors","title":"Data Operation exceptions","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"CollectionVersionMismatch","category":"page"},{"location":"errors/#DataToolkitCore.CollectionVersionMismatch","page":"Errors","title":"DataToolkitCore.CollectionVersionMismatch","text":"CollectionVersionMismatch(version::Int)\n\nThe version of the collection currently being acted on is not supported by the current version of DataToolkitCore.\n\nExample occurrence\n\njulia> fromspec(DataCollection, Dict{String, Any}(\"data_config_version\" => -1))\nERROR: CollectionVersionMismatch: -1 (specified) ≠ 0 (current)\n  The data collection specification uses the v-1 data collection format, however\n  the installed DataToolkitCore version expects the v0 version of the format.\n  In the future, conversion facilities may be implemented, for now though you\n  will need to manually upgrade the file to the v0 format.\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"EmptyStackError","category":"page"},{"location":"errors/#DataToolkitCore.EmptyStackError","page":"Errors","title":"DataToolkitCore.EmptyStackError","text":"EmptyStackError()\n\nAn attempt was made to perform an operation on a collection within the data stack, but the data stack is empty.\n\nExample occurrence\n\njulia> getlayer(nothing) # with an empty STACK\nERROR: EmptyStackError: The data collection stack is empty\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"ReadonlyCollection","category":"page"},{"location":"errors/#DataToolkitCore.ReadonlyCollection","page":"Errors","title":"DataToolkitCore.ReadonlyCollection","text":"ReadonlyCollection(collection::DataCollection)\n\nModification of collection is not viable, as it is read-only.\n\nExample Occurrence\n\njulia> lockedcollection = DataCollection(Dict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128)), \"config\" => Dict{String, Any}(\"locked\" => true)))\njulia> write(lockedcollection)\nERROR: ReadonlyCollection: The data collection unnamed#298 is locked\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"TransformerError","category":"page"},{"location":"errors/#DataToolkitCore.TransformerError","page":"Errors","title":"DataToolkitCore.TransformerError","text":"TransformerError(msg::String)\n\nA catch-all for issues involving data transformers, with details given in msg.\n\nExample occurrence\n\njulia> emptydata = DataSet(DataCollection(), \"empty\", Dict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128))))\nDataSet empty\n\njulia> read(emptydata)\nERROR: TransformerError: Data set \"empty\" could not be loaded in any form.\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"UnsatisfyableTransformer","category":"page"},{"location":"errors/#DataToolkitCore.UnsatisfyableTransformer","page":"Errors","title":"DataToolkitCore.UnsatisfyableTransformer","text":"UnsatisfyableTransformer{T}(dataset::DataSet, types::Vector{QualifiedType})\n\nA transformer (of type T) that could provide any of types was asked for, but there is no transformer that satisfies this restriction.\n\nExample occurrence\n\njulia> emptydata = DataSet(DataCollection(), \"empty\", Dict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128))))\nDataSet empty\n\njulia> read(emptydata, String)\nERROR: UnsatisfyableTransformer: There are no loaders for \"empty\" that can provide a String. The defined loaders are as follows:\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"OrphanDataSet","category":"page"},{"location":"errors/#DataToolkitCore.OrphanDataSet","page":"Errors","title":"DataToolkitCore.OrphanDataSet","text":"OrphanDataSet(dataset::DataSet)\n\nThe data set (dataset) is no longer a child of its parent collection.\n\nThis error should not occur, and is intended as a sanity check should something go quite wrong.\n\n\n\n\n\n","category":"type"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"EditURL=\"newtransformer.org\"","category":"page"},{"location":"newtransformer/#Creating-a-new-data-transformer","page":"Transformer backends","title":"Creating a new data transformer","text":"","category":"section"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"As mentioned before, there are three types of data transformer:","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"storage\nloader\nwriter","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"The three corresponding Julia types are:","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"DataStorage\nDataLoader\nDataWriter","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"All three types accept a driver (symbol) type parameter. For example, a storage transformer using a \"filesystem\" driver would be of the type DataStorage{:filesystem}.","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"Adding support for a new driver is a simple as adding method implementations for the three key data transformer methods:","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"load\nstorage\nsave","category":"page"},{"location":"newtransformer/#DataToolkitCore.load","page":"Transformer backends","title":"DataToolkitCore.load","text":"load(loader::DataLoader{driver}, source::Any, as::Type)\n\nUsing a certain loader, obtain information in the form of as from the data given by source.\n\nThis fulfils this component of the overall data flow:\n\n  ╭────loader─────╮\n  ╵               ▼\nData          Information\n\nWhen the loader produces nothing this is taken to indicate that it was unable to load the data for some reason, and that another loader should be tried if possible. This can be considered a soft failure. Any other value is considered valid information.\n\n\n\n\n\n","category":"function"},{"location":"newtransformer/#DataToolkitCore.storage","page":"Transformer backends","title":"DataToolkitCore.storage","text":"storage(storer::DataStorage, as::Type; write::Bool=false)\n\nFetch a storer in form as, appropiate for reading from or writing to (depending on write).\n\nBy default, this just calls getstorage or putstorage (when write=true).\n\nThis executes this component of the overall data flow:\n\nStorage ◀────▶ Data\n\n\n\n\n\n","category":"function"},{"location":"newtransformer/#DataToolkitCore.save","page":"Transformer backends","title":"DataToolkitCore.save","text":"save(writer::Datasaveer{driver}, destination::Any, information::Any)\n\nUsing a certain writer, save the information to the destination.\n\nThis fulfils this component of the overall data flow:\n\nData          Information\n  ▲               ╷\n  ╰────writer─────╯\n\n\n\n\n\n","category":"function"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"EditURL=\"libinternal.org\"","category":"page"},{"location":"libinternal/#Private-API","page":"Internals","title":"Private API","text":"","category":"section"},{"location":"libinternal/#Abstract-Data-Transformer","page":"Internals","title":"Abstract Data Transformer","text":"","category":"section"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"AbstractDataTransformer","category":"page"},{"location":"libinternal/#DataToolkitCore.AbstractDataTransformer","page":"Internals","title":"DataToolkitCore.AbstractDataTransformer","text":"The supertype for methods producing or consuming data.\n\n                 ╭────loader─────╮\n                 ╵               ▼\nStorage ◀────▶ Data          Information\n                 ▲               ╷\n                 ╰────writer─────╯\n\nThere are three subtypes:\n\nDataStorage\nDataLoader\nDataWrite\n\nEach subtype takes a Symbol type parameter designating the driver which should be used to perform the data operation. In addition, each subtype has the following fields:\n\ndataset::DataSet, the data set the method operates on\ntype::Vector{<:QualifiedType}, the Julia types the method supports\npriority::Int, the priority with which this method should be used, compared to alternatives. Lower values have higher priority.\nparameters::Dict{String, Any}, any parameters applied to the method.\n\n\n\n\n\n","category":"type"},{"location":"libinternal/#Advice-Amalgamation","page":"Internals","title":"Advice Amalgamation","text":"","category":"section"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"AdviceAmalgamation","category":"page"},{"location":"libinternal/#DataToolkitCore.AdviceAmalgamation","page":"Internals","title":"DataToolkitCore.AdviceAmalgamation","text":"A collection of Advices sourced from available Plugins.\n\nLike individual Advices, a AdviceAmalgamation can be called as a function. However, it also supports the following convenience syntax:\n\n(::AdviceAmalgamation)(f::Function, args...; kargs...) # -> result\n\nConstructors\n\nAdviceAmalgamation(advisors::Vector{Advice}, plugins_wanted::Vector{String}, plugins_used::Vector{String})\nAdviceAmalgamation(plugins::Vector{String})\nAdviceAmalgamation(collection::DataCollection)\n\n\n\n\n\n","category":"type"},{"location":"libinternal/#Qualified-Types","page":"Internals","title":"Qualified Types","text":"","category":"section"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"QualifiedType","category":"page"},{"location":"libinternal/#DataToolkitCore.QualifiedType","page":"Internals","title":"DataToolkitCore.QualifiedType","text":"A representation of a Julia type that does not need the type to be defined in the Julia session, and can be stored as a string. This is done by storing the type name and the module it belongs to as Symbols.\n\nwarning: Warning\nWhile QualifiedType is currently quite capable, it is not currently able to express the full gamut of Julia types. In future this will be improved, but it will likely always be restricted to a certain subset.\n\nSubtyping\n\nWhile the subtype operator cannot work on QualifiedTypes (<: is a built-in), when the Julia types are defined the subset operator ⊆ can be used instead. This works by simply converting the QualifiedTypes to the corresponding Type and then applying the subtype operator.\n\njulia> QualifiedTypes(:Base, :Vector) ⊆ QualifiedTypes(:Core, :Array)\ntrue\n\njulia> Matrix ⊆ QualifiedTypes(:Core, :Array)\ntrue\n\njulia> QualifiedTypes(:Base, :Vector) ⊆ AbstractVector\ntrue\n\njulia> QualifiedTypes(:Base, :Foobar) ⊆ AbstractVector\nfalse\n\nConstructors\n\nQualifiedType(parentmodule::Symbol, typename::Symbol)\nQualifiedType(t::Type)\n\nParsing\n\nA QualifiedType can be expressed as a string as \"$parentmodule.$typename\". This can be easily parsed as a QualifiedType, e.g. parse(QualifiedType, \"Core.IO\").\n\n\n\n\n\n","category":"type"},{"location":"libinternal/#Global-variables","page":"Internals","title":"Global variables","text":"","category":"section"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"STACK","category":"page"},{"location":"libinternal/#DataToolkitCore.STACK","page":"Internals","title":"DataToolkitCore.STACK","text":"The set of data collections currently available.\n\n\n\n\n\n","category":"constant"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"PLUGINS","category":"page"},{"location":"libinternal/#DataToolkitCore.PLUGINS","page":"Internals","title":"DataToolkitCore.PLUGINS","text":"The set of plugins currently available.\n\n\n\n\n\n","category":"constant"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"DataToolkitCore.EXTRA_PACKAGES","category":"page"},{"location":"libinternal/#DataToolkitCore.EXTRA_PACKAGES","page":"Internals","title":"DataToolkitCore.EXTRA_PACKAGES","text":"The set of packages loaded by each module via @addpkg, for import with @require.\n\nMore specifically, when a module M invokes @addpkg pkg id then EXTRA_PACKAGES[M][pkg] = id is set, and then this information is used with @require to obtain the package from the root module.\n\n\n\n\n\n","category":"constant"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"DATA_CONFIG_RESERVED_ATTRIBUTES","category":"page"},{"location":"libinternal/#DataToolkitCore.DATA_CONFIG_RESERVED_ATTRIBUTES","page":"Internals","title":"DataToolkitCore.DATA_CONFIG_RESERVED_ATTRIBUTES","text":"The data specification TOML format constructs a DataCollection, which itself contains DataSets, comprised of metadata and AbstractDataTransformers.\n\nDataCollection\n├─ DataSet\n│  ├─ AbstractDataTransformer\n│  └─ AbstractDataTransformer\n├─ DataSet\n⋮\n\nWithin each scope, there are certain reserved attributes. They are listed in this Dict under the following keys:\n\n:collection for DataCollection\n:dataset for DataSet\n:transformer for AbstractDataTransformer\n\n\n\n\n\n","category":"constant"},{"location":"packages/","page":"Packages","title":"Packages","text":"EditURL=\"packages.org\"","category":"page"},{"location":"packages/#Using-Packages","page":"Packages","title":"Using Packages","text":"","category":"section"},{"location":"packages/","page":"Packages","title":"Packages","text":"It is entirely likely that in the course of writing a package providing a custom data transformer, one would come across packages that may be needed.","category":"page"},{"location":"packages/","page":"Packages","title":"Packages","text":"Every possibly desired package could be shoved into the list of dependences, but this is a somewhat crude approach. A more granular approach is enabled with two macros, @addpkg and @require.","category":"page"},{"location":"packages/#Letting-DataToolkitCore-know-about-extra-packages","page":"Packages","title":"Letting DataToolkitCore know about extra packages","text":"","category":"section"},{"location":"packages/","page":"Packages","title":"Packages","text":"@addpkg","category":"page"},{"location":"packages/#DataToolkitCore.@addpkg","page":"Packages","title":"DataToolkitCore.@addpkg","text":"@addpkg name::Symbol uuid::String\n\nRegister the package identified by name with UUID uuid. This package may now be used with @require $name.\n\nAll @addpkg statements should lie within a module's __init__ function.\n\nExample\n\n@addpkg CSV \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\"\n\n\n\n\n\n","category":"macro"},{"location":"packages/#Using-extra-packages","page":"Packages","title":"Using extra packages","text":"","category":"section"},{"location":"packages/","page":"Packages","title":"Packages","text":"@require","category":"page"},{"location":"packages/#DataToolkitCore.@require","page":"Packages","title":"DataToolkitCore.@require","text":"@require Package\n@require Package = \"UUID\"\n\n\n\n\n\n","category":"macro"},{"location":"packages/#Example","page":"Packages","title":"Example","text":"","category":"section"},{"location":"packages/","page":"Packages","title":"Packages","text":"module DataToolkitExample\n\nusing DataToolkitCore\nusing DataFrame\n\nfunction __init__()\n    @addpkg CSV \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\"\n    @addpkg DelimitedFiles \"8bb1440f-4735-579b-a4ab-409b98df4dab\"\nend\n\nfunction load(::DataLoader{:csv}, from::IOStream, ::Type{DataFrame})\n    @require CSV\n    result = CSV.read(from, DataFrame)\n    close(from)\n    result\nend\n\nfunction load(::DataLoader{:delimcsv}, from::IOStream, ::Type{DataFrame})\n    @require DelimitedFiles\n    result = DelimitedFiles.readdlm(from, ',', DataFrame)\n    close(from)\n    result\nend\n\nend","category":"page"},{"location":"packages/","page":"Packages","title":"Packages","text":"Packages that implement loaders with other packages are recommended to use Julia 1.9's Package Extensions, together with the @requires macro and invokelatest like so:","category":"page"},{"location":"packages/","page":"Packages","title":"Packages","text":"# CsvLoaderPkg/src/loader.jl\nfunction load(::DataLoader{:csv}, from::IOStream, t::Type{DataFrame})\n    @require CSV\n    invokelatest(_load_csv, from, t)\nend","category":"page"},{"location":"packages/","page":"Packages","title":"Packages","text":"# CsvLoaderPkg/ext/csv.jl\nmodule csv\n\nusing CSV\nimport CsvLoaderPkg: _load_csv\n\nfunction _load_csv(from::IOStream, ::Type{DataFrame})\n    result = CSV.read(from, DataFrame)\n    close(from)\n    result\nend\n\nend","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"EditURL=\"index.org\"","category":"page"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"#The-problem-with-the-current-state-of-affairs","page":"Introduction","title":"The problem with the current state of affairs","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Data is beguiling. It can initially seem simple to deal with: \"here I have a file, and that's it\". However as soon as you do things with the data you're prone to be asked tricky questions like:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"where's the data?\nhow did you process that data?\nhow can I be sure I'm looking at the same data as you?","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This is no small part of the replication crisis.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: image)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Further concerns arise as soon as you start dealing with large quantities of data, or computationally expensive derived data sets. For example:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Have I already computed this data set somewhere else?\nIs my generated data up to date with its sources/dependencies?","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Generic tools exist for many parts of this problem, but there are some benefits that can be realised by creating a Julia-specific system, namely:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Having all pertinent environmental information in the data processing contained in a single Project.toml\nImproved convenience in data loading and management, compared to a generic solution\nAllowing datasets to be easily shared with a Julia package","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"In addition, the Julia community seems to have a strong tendency to NIH[NIH] tools, so we may as well get ahead of this and try to make something good 😛.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"[NIH]: Not Invented Here, a tendency to \"reinvent the wheel\" to avoid using tools from external origins — it would of course be better if you (re)made it.","category":"page"},{"location":"#Pre-existing-solutions","page":"Introduction","title":"Pre-existing solutions","text":"","category":"section"},{"location":"#DataLad","page":"Introduction","title":"DataLad","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Does a lot of things well\nPuts information on how to create data in git commit messages (bad)\nNo data file specification","category":"page"},{"location":"#Kedro-data-catalog","page":"Introduction","title":"Kedro data catalog","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Has a file defining all the data (good)\nHas poor versioning\nhttps://kedro.readthedocs.io/en/stable/data/data_catalog.html\nData Catalog CLI","category":"page"},{"location":"#Snakemake","page":"Introduction","title":"Snakemake","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Workflow manager, with remote file support\nSnakemake Remote Files\nGood list of possible file locations to handle\nDrawback is that you have to specify the location you expect(S3, http, FTP, etc.)\nNo data file specification","category":"page"},{"location":"#Nextflow","page":"Introduction","title":"Nextflow","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Workflow manager, with remote file support\nDocs on files and IO\nDocs on S3\nYou just call file() and nextflow figures out under the hood the protocol whether it should pull it from S3, http, FTP, or a local file.\nNo data file specification","category":"page"}]
}
